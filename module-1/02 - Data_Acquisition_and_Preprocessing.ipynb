{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2cc34b",
   "metadata": {},
   "source": [
    "![](../images/logos/KIEPSKIES.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01591f4",
   "metadata": {},
   "source": [
    "# <span style=\"color: #00008B;\"> Data Acquisition and Preprocessing</span>\n",
    "## <span style=\"color: #00008B;\"> Data Acquisition</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa757c05",
   "metadata": {},
   "source": [
    "Here are sources of data;\n",
    "\n",
    "1. Public data sets like Kaggle \n",
    "2. Trusted Sources like government websites and research organizations \n",
    "3. Company data - e.g operational data, manual data entry, sensors data\n",
    "4. Web APIs\n",
    "5. Web scraping \n",
    "6. Databases \n",
    "\n",
    "I. <span style=\"color: #00008B;\">**Public Sources**</span> \n",
    "\n",
    "Here is where you can find out free data for practice; \n",
    "\n",
    "* [Kaggle üèÜ](https://www.kaggle.com/datasets)\n",
    "* [Google Dataset Search üîç](https://archive.ics.uci.edu/datasets/)\n",
    "* [UCI Machine Learning Repository üéì](https://datasetsearch.research.google.com/)\n",
    "\n",
    "II. <span style=\"color: #00008B;\">**Trusted Sources for Raw Data**</span> \n",
    "\n",
    "Most government, NGOs have websites provide data to the public for research purposes (weather, tides, census) and public verification(doctors and laywer verifications). Some of the websites are listed below; \n",
    "\n",
    "- [GSP Monitoring Data](https://data.cnra.ca.gov/dataset/gspmd) - Groundwater Sustainability Plan (GSP) Monitoring dataset contains the monitoring sites and associated groundwater level, subsidence or streamflow measurements collected by Groundwater Sustainability Agencies (GSA) during implementation of their GSP. All data is submitted to the Department of Water Resources (DWR) through the Sustainable Groundwater Management Act (SGMA) Portal‚Äôs Monitoring Network Module (MNM). Is provided by CALIFORNIA NATURAL RESOURCES AGENCY. \n",
    "- [Doc Info](https://www.docinfo.org/) - provides data for qualified doctors, their state(location) and level of education. \n",
    "- [The Kenya National Bureau of Statistics](https://www.knbs.or.ke/)\n",
    "- [Kenya National Data Archive](https://statistics.knbs.or.ke/nada/index.php/catalog/179) (KeNADA)\n",
    "\n",
    "III. <span style=\"color: #00008B;\">**Company data**</span>\n",
    "\n",
    "ompanies collect data from various internal sources to monitor operations, optimize performance, and support decision-making. These sources can be categorized into the following:\n",
    "\n",
    "- Manual Data Entry - Data is manually inputted by employees, customers, or operators into spreadsheets, forms, or databases.Examples include Customer feedback forms,Employee attendance records and Sales logs entered by staff. **However this is prone to human error, time-consuming, and requires validation.**\n",
    "- Sensor Data (IoT & Automated Systems) - Data iscollected from sensors, Internet of Things (IoT) devices, and automated tracking systems in real time. Examples include; GPS tracking for ships in the fishing industry, Temperature & salinity sensors for ocean monitoring, Machine performance & maintenance logs in aquaculture farms. **However, it requires proper storage, data processing, and system integration.**\n",
    "- Company Operational Data - Data is generated through daily business activities, financial transactions, and resource management systems. Examples include; Inventory & supply chain data,Employee performance & payroll and Production & logistics reports. **However, it is often siloed across different departments, requiring data integration for full insights.**\n",
    "\n",
    "IV. <span style=\"color: #00008B;\">**Web APIs**</span>\n",
    "\n",
    "APIs (Application Programming Interfaces) are the easiest and most ethical way to get companies' data from data scientist. They allow us to request real-time data from organizations.\n",
    "\n",
    "Lets explore how to retrieve songs data from [Spotify](https://open.spotify.com/); \n",
    "\n",
    "The Spotify API Documentation can be accessed from [here](https://developer.spotify.com/documentation/web-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c87d289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: BQCHVEdiaC38S_6yqlzAFNbDQvzotOcBd8CG4mInjeygPOrFcuWHFPz8FoTipaxV4y9TVnKjSEckCOsMi084ix1D4Xad3yRl_WCpb9uYgJ7QbFaSgKcHJpV2xrr3B1CZb9KK62yTFoA\n"
     ]
    }
   ],
   "source": [
    "import requests  # Import the requests library for making HTTP requests\n",
    "\n",
    "# Define Spotify API credentials (Replace with your actual credentials)\n",
    "CLIENT_ID = '0d2cb3defcae451c8d935ff84090d752'\n",
    "CLIENT_SECRET = '323e1049ddc944d5a46fcca7c8b124e8'\n",
    "\n",
    "# URL for obtaining an access token\n",
    "AUTH_URL = 'https://accounts.spotify.com/api/token'\n",
    "\n",
    "# Send a POST request with credentials to get an access token\n",
    "auth_response = requests.post(AUTH_URL, {\n",
    "    'grant_type': 'client_credentials',  # Specify the authentication method\n",
    "    'client_id': CLIENT_ID,  # Pass the Client ID\n",
    "    'client_secret': CLIENT_SECRET,  # Pass the Client Secret\n",
    "})\n",
    "\n",
    "# Convert the response to JSON format\n",
    "auth_response_data = auth_response.json()\n",
    "\n",
    "# Extract and store the access token\n",
    "access_token = auth_response_data.get('access_token')\n",
    "\n",
    "# Print the access token (optional, for debugging purposes)\n",
    "print(\"Access Token:\", access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0754b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the authorization headers for API requests\n",
    "headers = {\n",
    "    'Authorization': 'Bearer {token}'.format(token=access_token)  # Add the access token to the header\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93c932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for all Spotify API endpoints\n",
    "BASE_URL = 'https://api.spotify.com/v1/'\n",
    "\n",
    "# Track ID for a specific song (Replace with any valid Spotify Track ID)\n",
    "track_id = '2TpxZ7JUBn3uw46aR7qd6V'\n",
    "\n",
    "# Send a GET request to fetch track details\n",
    "r = requests.get(BASE_URL + 'tracks/' + track_id, headers=headers)\n",
    "\n",
    "# Print the response (optional, for debugging)\n",
    "# print(r.json())  # Converts response to JSON and prints the track details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd20fbd",
   "metadata": {},
   "source": [
    "What Happens?\n",
    "\n",
    "- If the request is successful (status code 200), it returns track details like name, artist, album, and duration.\n",
    "- If there‚Äôs an error, it might return 401 (Unauthorized) if the token is invalid or expired.\n",
    "\n",
    "This request is a fundamental step in retrieving music metadata from Spotify‚Äôs API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf837fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = r.json()\n",
    "r.keys()\n",
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d034d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['href', 'limit', 'next', 'offset', 'previous', 'total', 'items'])\n"
     ]
    }
   ],
   "source": [
    "# Define the Artist ID (Example: Led Zeppelin)\n",
    "artist_id = '36QJpDe2go2KgaRleHCDTp'\n",
    "\n",
    "# Send a GET request to fetch all albums by the artist\n",
    "r = requests.get(\n",
    "    BASE_URL + 'artists/' + artist_id + '/albums',  # API endpoint for artist albums\n",
    "    headers=headers,  # Authorization headers with the access token\n",
    "    params={\n",
    "        'include_groups': 'album',  # Retrieve only full-length albums\n",
    "        'limit': 50  # Maximum number of albums to fetch per request\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert the response to JSON format\n",
    "d = r.json()\n",
    "\n",
    "# Print the JSON response (optional, for debugging)\n",
    "print(d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab924b5",
   "metadata": {},
   "source": [
    "V. <span style=\"color: #00008B;\">**Webscraping**</span>\n",
    "\n",
    "Web scraping is the process of extracting data from websites using automated scripts. It is commonly used for:\n",
    "\n",
    "- Gathering market intelligence (e.g., competitor prices, trends)\n",
    "- Extracting data for research (e.g., financial, weather, or sports data)\n",
    "- Collecting publicly available proxies, news, or social media content\n",
    "\n",
    "To perform web scraping, we use Python libraries like:\n",
    "\n",
    "- `requests` ‚Äì To send HTTP requests and retrieve webpage content\n",
    "- `BeautifulSoup` ‚Äì To parse and extract structured data from HTML\n",
    "- `pandas` ‚Äì To store and manipulate the extracted data in a DataFrame\n",
    "\n",
    "Here we will extract free proxies from  free-proxy-list.net and saves them in a pandas DataFrame. The following are key steps to be followed; \n",
    "\n",
    "i. Send an HTTP request to fetch the webpage's HTML content.\n",
    "ii. Parse the HTML using BeautifulSoup to find the relevant data.\n",
    "iii. Extract proxy details like IP address, port, country, and HTTPS support.\n",
    "iv. Store the extracted data in a structured pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be1295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ip_address port        country https_secured\n",
      "0  50.223.246.237   80  United States            no\n",
      "1    50.174.7.159   80  United States            no\n",
      "2  41.207.187.178   80           Togo            no\n",
      "3     32.223.6.94   80  United States            no\n",
      "4   82.119.96.254   80       Slovakia            no\n"
     ]
    }
   ],
   "source": [
    "import requests  # For sending HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import pandas as pd  # For storing extracted data in a structured format\n",
    "\n",
    "# Step 1: Define the target URL (Website with free proxy lists)\n",
    "url = 'https://free-proxy-list.net/'\n",
    "\n",
    "# Step 2: Send a GET request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Parse the HTML content of the webpage using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 4: Locate the table containing proxy information\n",
    "table = soup.find('table')  # Find the first table in the page\n",
    "table_body = table.find('tbody')  # Extract the body of the table\n",
    "\n",
    "# Step 5: Initialize lists to store extracted data\n",
    "ip_address = []   # List for storing IP addresses\n",
    "port = []         # List for storing port numbers\n",
    "country = []      # List for storing country names\n",
    "https_secured = []  # List for storing HTTPS support status\n",
    "\n",
    "# Step 6: Loop through each row in the table and extract data\n",
    "for tr in table_body.find_all('tr'):  # Iterate through each table row\n",
    "    td_s = tr.find_all('td')  # Extract all columns in the row\n",
    "\n",
    "    # Append extracted data to respective lists\n",
    "    ip_address.append(td_s[0].text)   # First column - IP Address\n",
    "    port.append(td_s[1].text)         # Second column - Port Number\n",
    "    country.append(td_s[3].text)      # Fourth column - Country\n",
    "    https_secured.append(td_s[6].text)  # Seventh column - HTTPS support (Yes/No)\n",
    "\n",
    "# Step 7: Create a pandas DataFrame to store the scraped data\n",
    "proxies_df = pd.DataFrame({\n",
    "    'ip_address': ip_address,\n",
    "    'port': port,\n",
    "    'country': country,\n",
    "    'https_secured': https_secured\n",
    "})\n",
    "\n",
    "# Step 8: Display the first 5 rows of the DataFrame\n",
    "print(proxies_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1506bbf",
   "metadata": {},
   "source": [
    "<span style=\"color: #00008B;\">*Ethics in Web Scraping*</span>\n",
    "\n",
    "Web scraping is a powerful technique for extracting data from websites, but it must be done ethically and legally to avoid violating terms of service or privacy laws. Here are key ethical considerations:\n",
    "\n",
    "- **Respect Website Terms of Service** - Many websites have a robots.txt file that outlines scraping rules.Some websites forbid scraping in their terms of service, and ignoring this could lead to legal action.\n",
    "- **Avoid Overloading Servers** - Sending too many requests in a short time can crash or slow down a website.Implement rate-limiting (e.g., adding delays between requests) to reduce strain.\n",
    "- **Do Not Scrape Personal or Sensitive Data** - Avoid scraping personal information (e.g., emails, passwords, medical records) unless explicitly permitted.Ensure compliance with data privacy laws like GDPR (Europe) and CCPA (California).\n",
    "- **Give Proper Credit** - If using scraped data for research, acknowledge the source. Do not misrepresent the data or plagiarize content.\n",
    "- **Use APIs When Available** - Many websites (e.g., Twitter, Spotify, OpenWeather) provide official APIs for data access. APIs are more stable, legal, and efficient than scraping.\n",
    "\n",
    "<span style=\"color: #00008B;\">*Limitations in Web Scraping*</span>\n",
    "\n",
    "Despite its advantages, web scraping has several limitations:\n",
    "\n",
    "1. Legal and Ethical Restrictions\n",
    "Some websites actively block scrapers using CAPTCHAs, IP bans, or legal action.\n",
    "Scraping copyrighted or proprietary data without permission can lead to lawsuits.\n",
    "2. Changing Website Structure\n",
    "Websites update their HTML frequently, breaking scrapers.\n",
    "Maintaining scrapers requires constant updates.\n",
    "3. Data Accuracy Issues\n",
    "Scraped data can contain errors, duplicates, or missing values.\n",
    "Parsing incorrect HTML may result in extracting wrong information.\n",
    "4. Performance and Cost Issues\n",
    "Large-scale scraping consumes bandwidth and computing power.\n",
    "Paid proxy services may be needed to bypass blocks, increasing costs.\n",
    "5. Limited Access to Dynamic Content\n",
    "Websites using JavaScript-rendered content (e.g., React, Vue) may require Selenium or Playwright, making scraping slower.\n",
    "API rate limits may restrict data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327ed24",
   "metadata": {},
   "source": [
    "V. <span style=\"color: #00008B;\">**Databases**</span>\n",
    "\n",
    "Databases store structured data that we can retrieve using SQL queries. In data science, we often need to extract this data into a pandas DataFrame for analysis. Python provides libraries like:\n",
    "\n",
    "- `sqlite3` ‚Äì To connect and interact with SQLite databases\n",
    "- `pandas` ‚Äì To store and manipulate extracted data in a DataFrame\n",
    "\n",
    "CRUD(Create, Read, Update, Delete) Operations can be performed on tables and records in data bases. \n",
    "\n",
    "Lets read three tables from `CWDatabase.db` that you will be provided by your instructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "008412d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artists Table:\n",
      "    ID       ArtistName\n",
      "0    1              DMX\n",
      "1    2  Destiny's Child\n",
      "2    3              112\n",
      "3    4          Afroman\n",
      "4    5            JAY-Z\n",
      "..  ..              ...\n",
      "57  58     Paulo Londra\n",
      "58  59           Polo G\n",
      "59  60          Stormzy\n",
      "60  61           DaBaby\n",
      "61  62             Sech\n",
      "\n",
      "[62 rows x 2 columns] \n",
      "\n",
      "Genre Table:\n",
      "    ID                                Genre\n",
      "0    1                         hip hop, pop\n",
      "1    2                             pop, R&B\n",
      "2    3                    hip hop, pop, R&B\n",
      "3    4                              hip hop\n",
      "4    5                pop, Dance/Electronic\n",
      "5    6                hip hop, pop, country\n",
      "6    7               World/Traditional, pop\n",
      "7    8                                  pop\n",
      "8    9  hip hop, pop, R&B, Dance/Electronic\n",
      "9   10          rock, pop, Dance/Electronic\n",
      "10  11     hip hop, latin, Dance/Electronic\n",
      "11  12            hip hop, Dance/Electronic\n",
      "12  13                                latin \n",
      "\n",
      "Songs Table:\n",
      "    ID                           Song  Duration  Explicit  Year  Popularity  \\\n",
      "0    1                       Party Up       269         1  1999          71   \n",
      "1    2                       Survivor       254         0  2001          70   \n",
      "2    3                Peaches & Cream       193         0  2001          63   \n",
      "3    4             Because I Got High       198         1  2001          68   \n",
      "4    5                Izzo (H.O.V.A.)       241         1  2001          63   \n",
      "..  ..                            ...       ...       ...   ...         ...   \n",
      "77  78                      Vossi Bop       196         1  2019          64   \n",
      "78  79  boyfriend (with Social House)       186         1  2019          77   \n",
      "79  80                           Suge       163         1  2019          72   \n",
      "80  81                  bury a friend       193         0  2019          75   \n",
      "81  82                     Otro Trago       226         1  2019          71   \n",
      "\n",
      "    Danceability  GenreID  ArtistID  \n",
      "0          0.510        1         1  \n",
      "1          0.514        2         2  \n",
      "2          0.677        3         3  \n",
      "3          0.802        4         4  \n",
      "4          0.618        4         5  \n",
      "..           ...      ...       ...  \n",
      "77         0.682       12        60  \n",
      "78         0.400        8        56  \n",
      "79         0.876        4        61  \n",
      "80         0.905        5        55  \n",
      "81         0.746       13        62  \n",
      "\n",
      "[82 rows x 9 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3  # SQLite database module\n",
    "import pandas as pd  # Pandas for data handling\n",
    "\n",
    "\n",
    "# Reconnect to database\n",
    "conn = sqlite3.connect('../data/database/CWDatabase.db')\n",
    "\n",
    "# Read the tables into pandas DataFrames\n",
    "df_artist = pd.read_sql_query(\"SELECT * FROM Artist\", conn)\n",
    "df_genre = pd.read_sql_query(\"SELECT * FROM Genre\", conn)\n",
    "df_song = pd.read_sql_query(\"SELECT * FROM Song\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Display the data\n",
    "print(\"Artists Table:\")\n",
    "print(df_artist, \"\\n\")\n",
    "\n",
    "print(\"Genre Table:\")\n",
    "print(df_genre, \"\\n\")\n",
    "\n",
    "print(\"Songs Table:\")\n",
    "print(df_song, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86eab0",
   "metadata": {},
   "source": [
    "## <span style=\"color: #00008B;\"> Data Preprocessing</span>\n",
    "\n",
    "### <span style=\"color: #00008B;\"> Data Cleaning</span>\n",
    "\n",
    "There is a separate notebook for this that will be provided by the instructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30c248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5bb4cc",
   "metadata": {},
   "source": [
    "### <span style=\"color: #00008B;\"> Pandas-profiling and Data Augmentation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7cdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
