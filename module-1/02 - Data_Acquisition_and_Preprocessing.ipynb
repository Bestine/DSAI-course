{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2cc34b",
   "metadata": {},
   "source": [
    "![](../images/logos/KIEPSKIES.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01591f4",
   "metadata": {},
   "source": [
    "# <span style=\"color: #00008B;\"> Data Acquisition and Preprocessing</span>\n",
    "## <span style=\"color: #00008B;\"> Data Acquisition</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa757c05",
   "metadata": {},
   "source": [
    "Here are sources of data;\n",
    "\n",
    "1. Public data sets like Kaggle \n",
    "2. Trusted Sources like government websites and research organizations \n",
    "3. Company data - e.g operational data, manual data entry, sensors data\n",
    "4. Web APIs\n",
    "5. Web scraping \n",
    "6. Databases \n",
    "\n",
    "I. <span style=\"color: #00008B;\">**Public Sources**</span> \n",
    "\n",
    "Here is where you can find out free data for practice; \n",
    "\n",
    "* [Kaggle üèÜ](https://www.kaggle.com/datasets)\n",
    "* [Google Dataset Search üîç](https://archive.ics.uci.edu/datasets/)\n",
    "* [UCI Machine Learning Repository üéì](https://datasetsearch.research.google.com/)\n",
    "\n",
    "II. <span style=\"color: #00008B;\">**Trusted Sources for Raw Data**</span> \n",
    "\n",
    "Most government, NGOs have websites provide data to the public for research purposes (weather, tides, census) and public verification(doctors and laywer verifications). Some of the websites are listed below; \n",
    "\n",
    "- [GSP Monitoring Data](https://data.cnra.ca.gov/dataset/gspmd) - Groundwater Sustainability Plan (GSP) Monitoring dataset contains the monitoring sites and associated groundwater level, subsidence or streamflow measurements collected by Groundwater Sustainability Agencies (GSA) during implementation of their GSP. All data is submitted to the Department of Water Resources (DWR) through the Sustainable Groundwater Management Act (SGMA) Portal‚Äôs Monitoring Network Module (MNM). Is provided by CALIFORNIA NATURAL RESOURCES AGENCY. \n",
    "- [Doc Info](https://www.docinfo.org/) - provides data for qualified doctors, their state(location) and level of education. \n",
    "- [The Kenya National Bureau of Statistics](https://www.knbs.or.ke/)\n",
    "- [Kenya National Data Archive](https://statistics.knbs.or.ke/nada/index.php/catalog/179) (KeNADA)\n",
    "\n",
    "III. <span style=\"color: #00008B;\">**Company data**</span>\n",
    "\n",
    "ompanies collect data from various internal sources to monitor operations, optimize performance, and support decision-making. These sources can be categorized into the following:\n",
    "\n",
    "- Manual Data Entry - Data is manually inputted by employees, customers, or operators into spreadsheets, forms, or databases.Examples include Customer feedback forms,Employee attendance records and Sales logs entered by staff. **However this is prone to human error, time-consuming, and requires validation.**\n",
    "- Sensor Data (IoT & Automated Systems) - Data iscollected from sensors, Internet of Things (IoT) devices, and automated tracking systems in real time. Examples include; GPS tracking for ships in the fishing industry, Temperature & salinity sensors for ocean monitoring, Machine performance & maintenance logs in aquaculture farms. **However, it requires proper storage, data processing, and system integration.**\n",
    "- Company Operational Data - Data is generated through daily business activities, financial transactions, and resource management systems. Examples include; Inventory & supply chain data,Employee performance & payroll and Production & logistics reports. **However, it is often siloed across different departments, requiring data integration for full insights.**\n",
    "\n",
    "IV. <span style=\"color: #00008B;\">**Web APIs**</span>\n",
    "\n",
    "APIs (Application Programming Interfaces) are the easiest and most ethical way to get companies' data from data scientist. They allow us to request real-time data from organizations.\n",
    "\n",
    "Lets explore how to retrieve songs data from [Spotify](https://open.spotify.com/); \n",
    "\n",
    "The Spotify API Documentation can be accessed from [here](https://developer.spotify.com/documentation/web-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c87d289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: BQB0-fhkG0UclZylR5XCAXQ1nT0xx_8JwEUyu0I21EPsBrLd6o4PdXBf3oNZR7g6iXdTMkv6MRBDXHcA_iuKQAlttd0WLOsLL8VWluF87z_eGPnRDcmXvYFBIuMhOpa71ys_DYn2Qpo\n"
     ]
    }
   ],
   "source": [
    "import requests  # Import the requests library for making HTTP requests\n",
    "\n",
    "# Define Spotify API credentials (Replace with your actual credentials)\n",
    "CLIENT_ID = '0d2cb3defcae451c8d935ff84090d752'\n",
    "CLIENT_SECRET = '323e1049ddc944d5a46fcca7c8b124e8'\n",
    "\n",
    "# URL for obtaining an access token\n",
    "AUTH_URL = 'https://accounts.spotify.com/api/token'\n",
    "\n",
    "# Send a POST request with credentials to get an access token\n",
    "auth_response = requests.post(AUTH_URL, {\n",
    "    'grant_type': 'client_credentials',  # Specify the authentication method\n",
    "    'client_id': CLIENT_ID,  # Pass the Client ID\n",
    "    'client_secret': CLIENT_SECRET,  # Pass the Client Secret\n",
    "})\n",
    "\n",
    "# Convert the response to JSON format\n",
    "auth_response_data = auth_response.json()\n",
    "\n",
    "# Extract and store the access token\n",
    "access_token = auth_response_data.get('access_token')\n",
    "\n",
    "# Print the access token (optional, for debugging purposes)\n",
    "print(\"Access Token:\", access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0754b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the authorization headers for API requests\n",
    "headers = {\n",
    "    'Authorization': 'Bearer {token}'.format(token=access_token)  # Add the access token to the header\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d93c932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for all Spotify API endpoints\n",
    "BASE_URL = 'https://api.spotify.com/v1/'\n",
    "\n",
    "# Track ID for a specific song (Replace with any valid Spotify Track ID)\n",
    "track_id = '2TpxZ7JUBn3uw46aR7qd6V'\n",
    "\n",
    "# Send a GET request to fetch track details\n",
    "r = requests.get(BASE_URL + 'tracks/' + track_id, headers=headers)\n",
    "\n",
    "# Print the response (optional, for debugging)\n",
    "# print(r.json())  # Converts response to JSON and prints the track details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd20fbd",
   "metadata": {},
   "source": [
    "What Happens?\n",
    "\n",
    "- If the request is successful (status code 200), it returns track details like name, artist, album, and duration.\n",
    "- If there‚Äôs an error, it might return 401 (Unauthorized) if the token is invalid or expired.\n",
    "\n",
    "This request is a fundamental step in retrieving music metadata from Spotify‚Äôs API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcf837fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = r.json()\n",
    "r.keys()\n",
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d034d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['href', 'limit', 'next', 'offset', 'previous', 'total', 'items'])\n"
     ]
    }
   ],
   "source": [
    "# Define the Artist ID (Example: Led Zeppelin)\n",
    "artist_id = '36QJpDe2go2KgaRleHCDTp'\n",
    "\n",
    "# Send a GET request to fetch all albums by the artist\n",
    "r = requests.get(\n",
    "    BASE_URL + 'artists/' + artist_id + '/albums',  # API endpoint for artist albums\n",
    "    headers=headers,  # Authorization headers with the access token\n",
    "    params={\n",
    "        'include_groups': 'album',  # Retrieve only full-length albums\n",
    "        'limit': 50  # Maximum number of albums to fetch per request\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert the response to JSON format\n",
    "d = r.json()\n",
    "\n",
    "# Print the JSON response (optional, for debugging)\n",
    "print(d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab924b5",
   "metadata": {},
   "source": [
    "V. <span style=\"color: #00008B;\">**Webscraping**</span>\n",
    "\n",
    "Web scraping is the process of extracting data from websites using automated scripts. It is commonly used for:\n",
    "\n",
    "- Gathering market intelligence (e.g., competitor prices, trends)\n",
    "- Extracting data for research (e.g., financial, weather, or sports data)\n",
    "- Collecting publicly available proxies, news, or social media content\n",
    "\n",
    "To perform web scraping, we use Python libraries like:\n",
    "\n",
    "- `requests` ‚Äì To send HTTP requests and retrieve webpage content\n",
    "- `BeautifulSoup` ‚Äì To parse and extract structured data from HTML\n",
    "- `pandas` ‚Äì To store and manipulate the extracted data in a DataFrame\n",
    "\n",
    "Here we will extract free proxies from  free-proxy-list.net and saves them in a pandas DataFrame. The following are key steps to be followed; \n",
    "\n",
    "i. Send an HTTP request to fetch the webpage's HTML content.\n",
    "ii. Parse the HTML using BeautifulSoup to find the relevant data.\n",
    "iii. Extract proxy details like IP address, port, country, and HTTPS support.\n",
    "iv. Store the extracted data in a structured pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4be1295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ip_address port        country https_secured\n",
      "0    85.215.64.49   80        Germany            no\n",
      "1     74.48.78.52   80  United States            no\n",
      "2  50.223.246.237   80  United States            no\n",
      "3    50.174.7.159   80  United States            no\n",
      "4  41.207.187.178   80           Togo            no\n"
     ]
    }
   ],
   "source": [
    "import requests  # For sending HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import pandas as pd  # For storing extracted data in a structured format\n",
    "\n",
    "# Step 1: Define the target URL (Website with free proxy lists)\n",
    "url = 'https://free-proxy-list.net/'\n",
    "\n",
    "# Step 2: Send a GET request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Parse the HTML content of the webpage using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 4: Locate the table containing proxy information\n",
    "table = soup.find('table')  # Find the first table in the page\n",
    "table_body = table.find('tbody')  # Extract the body of the table\n",
    "\n",
    "# Step 5: Initialize lists to store extracted data\n",
    "ip_address = []   # List for storing IP addresses\n",
    "port = []         # List for storing port numbers\n",
    "country = []      # List for storing country names\n",
    "https_secured = []  # List for storing HTTPS support status\n",
    "\n",
    "# Step 6: Loop through each row in the table and extract data\n",
    "for tr in table_body.find_all('tr'):  # Iterate through each table row\n",
    "    td_s = tr.find_all('td')  # Extract all columns in the row\n",
    "\n",
    "    # Append extracted data to respective lists\n",
    "    ip_address.append(td_s[0].text)   # First column - IP Address\n",
    "    port.append(td_s[1].text)         # Second column - Port Number\n",
    "    country.append(td_s[3].text)      # Fourth column - Country\n",
    "    https_secured.append(td_s[6].text)  # Seventh column - HTTPS support (Yes/No)\n",
    "\n",
    "# Step 7: Create a pandas DataFrame to store the scraped data\n",
    "proxies_df = pd.DataFrame({\n",
    "    'ip_address': ip_address,\n",
    "    'port': port,\n",
    "    'country': country,\n",
    "    'https_secured': https_secured\n",
    "})\n",
    "\n",
    "# Step 8: Display the first 5 rows of the DataFrame\n",
    "print(proxies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d950578",
   "metadata": {},
   "outputs": [],
   "source": [
    "V. <span style=\"color: #00008B;\">**Databases**</span>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
