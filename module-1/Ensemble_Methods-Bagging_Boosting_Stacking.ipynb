{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec08bb69-53a8-49a5-8845-3d604c1c1ee8",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "Ensemble methods combine multiple models to improve accuracy and robustness. The main types are:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating)\n",
    "2. Boosting (Sequential Learning)\n",
    "3. Stacking (Blending Multiple Models)\n",
    "\n",
    "We'll demonstrate these using sklearn on the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785d045-c607-4da1-9e72-e6258cfec9b8",
   "metadata": {},
   "source": [
    "Lets load the data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd27c5a7-10f5-4b08-832e-4ade5629e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de650e-e243-4f6f-bd4b-3b81509e3432",
   "metadata": {},
   "source": [
    "<span style=\"color: #00008B;\">**Bagging (Bootstrap Aggregating)**</span>\n",
    "\n",
    "Bagging reduces variance by training multiple models on random subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621d5cb0-e5a4-434a-a8bf-8d6dee651a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Bagging using Decision Tree\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Bagging Accuracy: {accuracy_score(y_test, y_pred_bagging):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c062a-5035-49be-8183-107844ac415b",
   "metadata": {},
   "source": [
    "This how bagging works; \n",
    "\n",
    "- Uses multiple independent models (often the same type).\n",
    "- Reduces variance (helps in overfitting).\n",
    "\n",
    "Example: Random Forest is an extension of bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84154133-504e-4bf4-9568-abb9dbb36792",
   "metadata": {},
   "source": [
    "<span style=\"color: #00008B;\">**Boosting (Sequential Learning)**</span>\n",
    "\n",
    "Boosting trains models sequentially, where each model corrects previous errors.\n",
    "\n",
    "a. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost assigns higher weights to misclassified instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4207340-491c-44d4-a56b-76f5b7e244e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost with Decision Trees\n",
    "adaboost_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, y_pred_adaboost):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32718f22-8ec0-4f03-9118-99dd0a4d02d3",
   "metadata": {},
   "source": [
    "b. Gradient Boosting\n",
    "\n",
    "Gradient Boosting minimizes errors using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cf40cf-f0cf-47af-872f-34c0cb5c4b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred_gb):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad68fd-7891-4aab-a7da-f8523674a50d",
   "metadata": {},
   "source": [
    "Here is how bagging works; \n",
    "\n",
    "- Boosting focuses on hard-to-classify examples.\n",
    "- Stronger than bagging but prone to overfitting.\n",
    "\n",
    "Example: XGBoost, LightGBM, CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36763fd-bed8-4983-b999-45b66be462ad",
   "metadata": {},
   "source": [
    "<span style=\"color: #00008B;\">**Stacking (Blending Multiple Models)**</span>\n",
    "\n",
    "Stacking combines different models and uses a meta-model for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baba181d-0f19-4790-918e-acbdc0a052c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Define base models\n",
    "estimators = [\n",
    "    ('rf', DecisionTreeClassifier()),\n",
    "    ('svc', SVC(probability=True))\n",
    "]\n",
    "\n",
    "# Stacking Classifier with Logistic Regression as meta-model\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Stacking Accuracy: {accuracy_score(y_test, y_pred_stacking):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc77e8-5ed2-4a70-b332-f6926a9c7d8d",
   "metadata": {},
   "source": [
    "Stacking combines predictions from different models. It is the strongest in prediction acuracy but computationally expensive. Uses a meta-model(.eg. logitistic regression)\n",
    "\n",
    "|Method\t| Description | Example Models|\n",
    "|:------:|:------:|:-------:|\n",
    "|Bagging | Uses multiple independent models to reduce variance.\t| Random Forest|\n",
    "|Boosting | Sequentially learns from errors to improve accuracy.|\tAdaBoost, Gradient Boosting, XGBoost|\n",
    "|Stacking |\tCombines different models using a meta-learner.\t| Blending multiple classifiers|\n",
    "\n",
    "\n",
    "When faced with a an ML problem here is a brief guide to check on; \n",
    "\n",
    "- Bagging: If overfitting is an issue (Random Forest).\n",
    "- Boosting: If high accuracy is required (XGBoost, LightGBM).\n",
    "- Stacking: If multiple diverse models work well together.\n",
    "\n",
    "However, ty experimenting with different models for better results!ðŸš€ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
