{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5042ff3d-700c-4084-9d5a-f81e1ed97666",
   "metadata": {},
   "source": [
    "# Web Scraping \n",
    "## Introduction to Web Scraping\n",
    "\n",
    "Web scraping is the process of extracting data from websites automatically using code. Instead of manually copying and pasting information, web scraping allows us to collect data efficiently and systematically.\n",
    "\n",
    "Why Web Scraping?\n",
    "Many websites contain valuable data that can be used for analysis, research, or automation. Some common applications of web scraping include:\n",
    "\n",
    "- Collecting product prices from e-commerce sites üìä\n",
    "- Gathering real estate listings for market analysis üè†\n",
    "- Extracting news articles for sentiment analysis üì∞\n",
    "- Scraping job listings for employment trends üíº\n",
    "\n",
    "**How Web Scraping Works**\n",
    "\n",
    "At a high level, web scraping involves:\n",
    "\n",
    "Sending an HTTP request ‚Äì Accessing a webpage using Python libraries like requests.\n",
    "Parsing the HTML content ‚Äì Extracting the required data using tools like BeautifulSoup or lxml.\n",
    "Navigating through the webpage structure ‚Äì Identifying elements like headings, tables, and links.\n",
    "Storing the extracted data ‚Äì Saving the information in a structured format such as CSV, JSON, or a database.\n",
    "Legal and Ethical Considerations\n",
    "Before scraping a website, always check:\n",
    "\n",
    "- robots.txt file ‚Äì Websites specify which parts of their site can be scraped.\n",
    "- Terms of service ‚Äì Some websites prohibit automated data collection.\n",
    "- Ethical use ‚Äì Only scrape publicly available data and avoid overloading a website with frequent requests.\n",
    "\n",
    "In the next section, we'll set up our Python environment and start scraping real data! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75dd2e-b668-452a-919b-ea3ab0cc01b6",
   "metadata": {},
   "source": [
    "## Web Scraping in Action\n",
    "### Installing Web Scraping Libraries in Python\n",
    "\n",
    "Before we start web scraping, we need to install some essential libraries that will help us extract data from websites. These libraries include:\n",
    "\n",
    "- `requests`: Used for making HTTP requests to web pages and retrieving their content.\n",
    "- `beautifulsoup4`: A library for parsing HTML and XML documents, making it easy to extract specific data.\n",
    "- `selenium`: A powerful tool for automating web browsers, which is useful for scraping websites that require interaction (e.g., clicking buttons, handling JavaScript-rendered content).\n",
    "\n",
    "To install these libraries, run the following command in your terminal or command prompt:\n",
    "```bash\n",
    "pip install requests beautifulsoup4 selenium\n",
    "```\n",
    "\n",
    "Once installed, you can check if the libraries are properly installed by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3845f86a-c3ed-4308-9152-638092bdcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "print(\"Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f88ea0-3fb3-4c45-98f5-a5cae2306dd2",
   "metadata": {},
   "source": [
    "## Connect to the target URL\n",
    "\n",
    "Now that we have installed the necessary libraries, let‚Äôs start by making a simple web request to a website.\n",
    "\n",
    "The requests library allows us to send HTTP requests to a website and retrieve its HTML content.\n",
    "\n",
    "The following code sends a request to the website quotes.toscrape.com, which is a test site designed for practicing web scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a561cbd4-0200-42f8-a2d1-baf2175f4767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "# Define the target URL  \n",
    "url = \"https://quotes.toscrape.com\"  \n",
    "\n",
    "# Send a GET request to the website  \n",
    "page = requests.get(url)  \n",
    "\n",
    "# Print the status code of the response  \n",
    "print(f\"Status Code: {page.status_code}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b992f-fcb9-465e-a074-29965c36941b",
   "metadata": {},
   "source": [
    "This is what happens on the above code cell; \n",
    "\n",
    "- `requests.get(url)`: Sends a request to the website and retrieves the response.\n",
    "- `page.status_code`: Displays the status of the request.\n",
    " \n",
    "\t- `200` means the request was successful.\n",
    "\t- `404` means the page was not found.\n",
    "In our case the status code was `200` therefore we successfully accessed the website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb95e2-c6c7-45da-adc8-733253a6433f",
   "metadata": {},
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "Now that we have retrieved the webpage content using requests, we need to parse the HTML so that we can extract useful information. This is where BeautifulSoup comes in!\n",
    "\n",
    "**What is BeautifulSoup?**\n",
    "\n",
    "BeautifulSoup is a Python library used for parsing HTML and XML documents. It allows us to navigate and search through the webpage‚Äôs structure easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a07236-b47a-48a3-8d0c-5e25d9e9abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Quotes to Scrape\n",
      "  </title>\n",
      "  <link href=\"/static/bootstrap.min.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"/static/main.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <div class=\"container\">\n",
      "   <div class=\"row header-box\">\n",
      "    <div class=\"col-md-8\">\n",
      "     <h1>\n",
      "      <a href=\"/\" style=\"text-decoration: none\">\n",
      "       Quotes to Scrape\n",
      "      </a>\n",
      "     </h1>\n",
      "    </div>\n",
      "    <div class=\"col-md-4\">\n",
      "     <p>\n",
      "      <a href=\"/login\">\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Parse the page content using BeautifulSoup  \n",
    "soup = BeautifulSoup(page.text, 'html.parser')  \n",
    "\n",
    "# Print the formatted HTML content  \n",
    "print(soup.prettify()[:500])  # Display only the first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c62431-e83d-48ea-9474-6b1ffac6f6ec",
   "metadata": {},
   "source": [
    "From the above code; \n",
    "\n",
    "- `BeautifulSoup(page.text, 'html.parser`:\n",
    "\n",
    "\t- Converts the HTML content (`page.text`) into a structured format that we can navigate.\n",
    "\t- `'html.parser'` is the built-in parser in Python.\n",
    "\n",
    "- `soup.prettify()`:\n",
    "\n",
    "\t- Formats the HTML in a readable structure.\n",
    "\t- We use `[:500]` to show only the first 500 characters (to avoid printing too much data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a3273-4e82-4c92-b663-5a3062c0bde9",
   "metadata": {},
   "source": [
    "## Select HTML elements with Beautiful Soup\n",
    "\n",
    "Now that we have parsed the HTML content using BeautifulSoup, let‚Äôs explore how to extract specific elements from the page.\n",
    "\n",
    "1. **Finding Elements by Tag Name**\n",
    "\n",
    "To retrieve all elements of a particular tag, such as `<h1>`, use `find_all()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd84181-4556-4000-84fa-b98e5f4591fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1>\n",
      "<a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "</h1>]\n"
     ]
    }
   ],
   "source": [
    "# Get all <h1> elements on the page\n",
    "h1_elements = soup.find_all('h1')\n",
    "print(h1_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126e5a8-b1ac-41d5-8bcc-b0a201bddc95",
   "metadata": {},
   "source": [
    "This returns a list of all `<h1>` elements present in the HTML.\n",
    "\n",
    "2. **Finding Elements by ID**\n",
    "\n",
    "HTML elements often have unique `id` attributes. You can retrieve an element by its `id` using `find()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5062ec32-6b1d-437a-be27-9fa87c00e9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get the element with id=\"main-title\"\n",
    "main_title_element = soup.find(id='main-title')\n",
    "print(main_title_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71274c-e143-4838-a175-2f0a76c6274d",
   "metadata": {},
   "source": [
    "3. **Finding Elements by Text Content**\n",
    "\n",
    "Sometimes, elements don‚Äôt have specific attributes, but we can identify them based on their text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e313e6-c9a5-48f6-977e-300556942960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16981/97244425.py:2: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  footer_element = soup.find(text='Powered by WordPress')\n"
     ]
    }
   ],
   "source": [
    "# Find the footer element based on the text it contains\n",
    "footer_element = soup.find(text='Powered by WordPress')\n",
    "print(footer_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ed1fa-dc0d-4c92-8c94-f2793c886085",
   "metadata": {},
   "source": [
    "4. **Finding Elements by Attribute**\n",
    "\n",
    "If an element has a unique attribute, we can search for it using `attrs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0bcaf70-ff73-4d66-a511-019b8ad2fd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Find the email input element through its \"name\" attribute\n",
    "email_element = soup.find(attrs={'name': 'email'})\n",
    "print(email_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca3d92-bdf2-4414-b5af-a8c335cbf619",
   "metadata": {},
   "source": [
    "5. **Finding Elements by Class**\n",
    "\n",
    "To find elements by class name, use the `class_` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fb2de8-a00a-495c-929d-13884cbb8d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Find all the centered elements on the page\n",
    "centered_elements = soup.find_all(class_='text-center')\n",
    "print(centered_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68321d13-b33a-416f-8227-ee81c0d4387a",
   "metadata": {},
   "source": [
    "6. **Combining Methods for Precise Selection**\n",
    "    \n",
    "We can chain methods to extract elements within specific sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ece4d4-89fe-4aba-9ed7-c6534e8edec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all \"li\" elements inside the \".navbar\" element\n",
    "# navbar_items = soup.find(class_='navbar').find_all('li')\n",
    "# print(navbar_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a9302-9da6-48b5-ba79-4e600da1c6a4",
   "metadata": {},
   "source": [
    "7. **Using CSS Selectors with select()**\n",
    "\n",
    "Instead of using multiple `.find()` calls, we can use CSS selectors with `select()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3a9177-bedc-4b07-9940-d5fa40c530ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Get all \"li\" elements inside the \".navbar\" element\n",
    "navbar_items = soup.select('.navbar > li')\n",
    "print(navbar_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106cf6e7-86de-4bb3-a89c-8df354c9a3c3",
   "metadata": {},
   "source": [
    "**In Summary**\n",
    "\n",
    "- `find_all('tag')` ‚Üí Retrieves all elements of a given tag.\n",
    "- `find(id='some_id')` ‚Üí Finds an element by its unique ID.\n",
    "- `find(text='some_text')` ‚Üí Finds an element based on text content.\n",
    "- `find(attrs={'name': 'some_name'})` ‚Üí Searches for an element by attribute.\n",
    "- `find_all(class_='some_class')` ‚Üí Retrieves all elements with a specific class.\n",
    "- `soup.select('CSS_selector')` ‚Üí Uses CSS selectors for more flexible selection.\n",
    "\n",
    "Using these methods, we can precisely extract any HTML element from a webpage and manipulate it as needed! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cabcf5-c35f-495f-801e-5b1b2a225dc4",
   "metadata": {},
   "source": [
    "## Extract data from the elements\n",
    "\n",
    "Now that we have learned how to navigate and extract elements from an HTML page, let's store the scraped data in a structured format.\n",
    "\n",
    "1. **Initializing a Data Structure**\n",
    "\n",
    "Before extracting data, we need a storage structure. Since we are working with multiple quotes, we‚Äôll use a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f410241-dbb4-4c9e-b60a-6b9594e618cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the scraped quotes\n",
    "quotes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bbad04-748a-47fe-bac4-5ec17001c3d6",
   "metadata": {},
   "source": [
    "2. **Finding the Quote Elements**\n",
    "\n",
    "Each quote on the page is enclosed in a `<div>` with the class `\"quote\"`. We use `find_all()` to get all such elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fffe081-3915-446d-b9b1-a0b635377bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <div> elements with the class \"quote\"\n",
    "quote_elements = soup.find_all('div', class_='quote')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd30bd-4665-42c6-ad55-063e19cb7bd7",
   "metadata": {},
   "source": [
    "Now, `quote_elements` contains a list of all quotes on the page.\n",
    "\n",
    "3. **Extracting Quote Data**\n",
    "\n",
    "To retrieve the text, author, and tags for each quote, iterate over the list and extract the relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229bb3a1-c757-40c0-958d-abb56c036ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote_element in quote_elements:\n",
    "    # Extract the quote text\n",
    "    text = quote_element.find('span', class_='text').text\n",
    "\n",
    "    # Extract the author of the quote\n",
    "    author = quote_element.find('small', class_='author').text\n",
    "\n",
    "    # Extract the tag elements associated with the quote\n",
    "    tag_elements = quote_element.select('.tags .tag')\n",
    "\n",
    "    # Store the tags in a list\n",
    "    tags = []\n",
    "    for tag_element in tag_elements:\n",
    "        tags.append(tag_element.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf652ecf-f24c-411f-abdb-253c97f3691d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['humor', 'obvious', 'simile']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe62b8c-c97c-4e19-93c9-3802fbc76eba",
   "metadata": {},
   "source": [
    "4. **Storing Data in a Dictionary**\n",
    "\n",
    "After extracting the necessary details, we store each quote as a dictionary and add it to our list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e38130a4-7433-4536-bead-18cf9006b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append extracted data to the quotes list\n",
    "quotes.append(\n",
    "    {\n",
    "        'text': text,\n",
    "        'author': author,\n",
    "        'tags': ', '.join(tags)  # Convert list of tags into a single string\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4134559c-a586-480a-83a5-cf9ec9351b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '‚ÄúA day without sunshine is like, you know, night.‚Äù',\n",
       "  'author': 'Steve Martin',\n",
       "  'tags': 'humor, obvious, simile'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e8457-40b1-41ea-99ea-eb356464bb8b",
   "metadata": {},
   "source": [
    "Now, quotes is a list of dictionaries, each containing:\n",
    "\n",
    "- `\"text\"` ‚Üí The quote itself\n",
    "- `\"author\"` ‚Üí The person who said it\n",
    "- `\"tags\"` ‚Üí Relevant topics\n",
    "  \n",
    "5. **Printing the Scraped Data**\n",
    "\n",
    "To verify the extracted data, print a few quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b793f57-2fdd-4bcf-ba37-3857166c2d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '‚ÄúA day without sunshine is like, you know, night.‚Äù', 'author': 'Steve Martin', 'tags': 'humor, obvious, simile'}\n"
     ]
    }
   ],
   "source": [
    "# Display first 5 quotes\n",
    "for quote in quotes[:5]:\n",
    "    print(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305f62d-4c04-4106-8530-9e4105069110",
   "metadata": {},
   "source": [
    "## Exporting Scraped Data to a CSV File\n",
    "\n",
    "Once we have successfully extracted quotes from the website, the next step is to save them in a structured format. One of the most common ways to store tabular data is in a CSV (Comma-Separated Values) file.\n",
    "\n",
    "Python provides a built-in `csv` module, which makes it easy to write data to a CSV file. First, we import the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64570e24-f344-430e-b238-96cda1e157be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87111c62-396e-4dae-a15c-eed8308916c5",
   "metadata": {},
   "source": [
    "We use the `open()` function to create (or overwrite) a CSV file named `\"quotes.csv\"`. The 'w' mode ensures that we write to a new file.\n",
    "\n",
    "- `'w'` ‚Üí Write mode (creates a new file or overwrites an existing one).\n",
    "- `encoding='utf-8'` ‚Üí Ensures proper handling of special characters.\n",
    "- `newline=''` ‚Üí Prevents extra blank lines when writing to the file (especially on Windows).\n",
    "\n",
    "After opening the file, we initialize the CSV writer\n",
    "\n",
    "The `writer` object helps insert data into the CSV file.\n",
    "\n",
    "The first row of a CSV file typically contains column headers. Since we extracted `text`, `author`, and `tags`, we define our headers\n",
    "\n",
    "We iterate over our quotes list and write each quote to a new row:\n",
    "\n",
    "- `quote.values()` retrieves the dictionary values (text, author, and tags).\n",
    "- The `writer.writerow()` method writes each quote as a new row in the CSV file.\n",
    "\n",
    "\n",
    "Since we used a with `open(...)` as `csv_file`: block, the file automatically closes after execution, freeing system resources. However, if you use `open()` without `with`, you should manually close the file using: `csv_file.close()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c0d62a1-551a-4a04-ae36-b2ae72f53487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading  the \"quotes.csv\" file and creating it\n",
    "# if not present\n",
    "with open('../data/quotes.csv', 'w', encoding='utf-8', newline='') as csv_file:\n",
    "\t# initializing the writer object to insert data\n",
    "\t# in the CSV file\n",
    "\twriter = csv.writer(csv_file)\n",
    "\n",
    "\t# writing the header of the CSV file\n",
    "\twriter.writerow(['Text', 'Author', 'Tags'])\n",
    "\n",
    "\t# writing each row of the CSV\n",
    "\tfor quote in quotes:\n",
    "\t    writer.writerow(quote.values())\n",
    "\n",
    "# terminating the operation and releasing the resources\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721fee4-f74b-47bd-b3db-122e934d0457",
   "metadata": {},
   "source": [
    "After running the script, check your project folder. You should see a quotes.csv file containing the quote:\n",
    "```\n",
    "‚ÄúA day without sunshine is like, you know, night.‚Äù\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
